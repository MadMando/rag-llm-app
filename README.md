
# ğŸ§  RAG LLM App (Local & Multi-User)

A fully local, multi-user **Retrieval-Augmented Generation (RAG)** application powered by **Langflow**, **ChromaDB**, **Ollama**, and **FastAPI**. This project allows you to load documents, generate embeddings, store in a vector database, and query them using a local LLM â€” all served via an API for concurrent access.

## ğŸš€ Features

- ğŸ” Search across your own documents (`.pdf`, `.txt`)
- ğŸ§  Local LLM inference with quantized models (via Ollama)
- ğŸ—ƒï¸ Vector storage and retrieval using ChromaDB
- âš¡ FastAPI backend with multi-user support
- ğŸ–¼ï¸ Langflow UI for no-code visual pipelines

## ğŸ› ï¸ Tech Stack

| Layer         | Tool               |
|---------------|--------------------|
| LLM           | Ollama (`llama3`)  |
| Embeddings    | Ollama (`nomic-embed-text`) |
| Vector DB     | ChromaDB           |
| Orchestration | Langflow           |
| API Server    | FastAPI            |
| Environment   | Python 3.12 (Miniconda) |

## ğŸ—‚ï¸ Project Structure

```
rag-llm-app/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ main.py          # FastAPI app
â”œâ”€â”€ data/                # Drop .pdf or .txt documents here
â”œâ”€â”€ langflow/            # Langflow project files (optional)
â”œâ”€â”€ requirements.txt     # Python dependencies (except langflow/uv)
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## âš™ï¸ Quickstart Guide (via Miniconda)

> Full setup instructions and Langflow flows are available in the repo:  
> ğŸ‘‰ [https://github.com/MadMando/rag-llm-app](https://github.com/MadMando/rag-llm-app)

### 1ï¸âƒ£ Install Miniconda

Download Miniconda from:  
ğŸ”— https://docs.conda.io/en/latest/miniconda.html

### 2ï¸âƒ£ Install Ollama and Pull Models

Download Ollama for your OS:  
ğŸ”— https://ollama.com/download

Then open a terminal or command prompt and run:

```bash
ollama pull llama3             # LLM model
ollama pull nomic-embed-text   # Embedding model
```

Make sure Ollama is running after installation.

### 3ï¸âƒ£ Set Up the Project Environment

```bash
# Clone the repo
git clone https://github.com/MadMando/rag-llm-app.git
cd rag-llm-app

# Create and activate your conda environment
conda create -n rag-env python=3.12 -y
conda activate rag-env

# Install uv and langflow
pip install uv
uv pip install langflow

# Install all other dependencies
uv pip install -r requirements.txt
```

### 4ï¸âƒ£ Start the RAG Stack

```bash
# Start ChromaDB
chroma run --path ./data/chroma_data

# Start Langflow
langflow run
```

Langflow will open at:  
ğŸ‘‰ http://localhost:7860

### 5ï¸âƒ£ (Optional) Run FastAPI App

```bash
uvicorn app.main:app --reload
```

Visit the docs:  
ğŸ‘‰ http://localhost:8000/docs

### ğŸ“‚ Document Ingestion

Drop your `.pdf` and `.txt` files into the `data/` folder.  
Langflow will use them during your flow execution with ChromaDB.

## âœ… Status

- âœ… Local LLMs via Ollama  
- âœ… Langflow integration  
- âœ… Multi-user query support via FastAPI  
- â³ Next: Add UI chat interface + advanced chunking logic

## ğŸ“„ License

MIT License

## âœï¸ Author

Built by [Armando Medina](https://www.linkedin.com/in/armandomedina)  
Follow for more projects on GenAI, RAG, and LLM applications.
