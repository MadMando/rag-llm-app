# ğŸ§  RAG LLM App (Local & Multi-User)

A fully local, multi-user **Retrieval-Augmented Generation (RAG)** application powered by **Langflow**, **ChromaDB**, and **Ollama**. This project allows you to load documents, generate embeddings, store them in a vector database, and query them using a local LLM â€” all within a visually interactive pipeline.

## ğŸš€ Features

- ğŸ” Search across your own documents (`.pdf`, `.txt`)
- ğŸ§  Local LLM inference with quantized models (via Ollama)
- ğŸ—ƒï¸ Vector storage and retrieval using ChromaDB
- ğŸ–¼ï¸ Langflow UI for no-code visual pipelines

## ğŸ› ï¸ Tech Stack

| Layer         | Tool               |
|---------------|--------------------|
| LLM           | Ollama (`llama3`)  |
| Embeddings    | Ollama (`nomic-embed-text`) |
| Vector DB     | ChromaDB           |
| Orchestration | Langflow           |
| Environment   | Python 3.12 (Miniconda) |

## ğŸ—‚ï¸ Project Structure

```
rag-llm-app/
â”œâ”€â”€ data/                # Drop .pdf or .txt documents here
â”œâ”€â”€ langflow/            # Langflow project files (optional)
â”œâ”€â”€ requirements.txt     # Python dependencies (except langflow/uv)
â”œâ”€â”€ Index.html           # Sample HTML index file with chat widget
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## âš™ï¸ Quickstart Guide (via Miniconda)

> Full setup instructions and Langflow flows are available in the repo:  
> ğŸ‘‰ [https://github.com/MadMando/rag-llm-app](https://github.com/MadMando/rag-llm-app)

### 1ï¸âƒ£ Install Miniconda

Download Miniconda from:  
ğŸ”— https://docs.conda.io/en/latest/miniconda.html

### 2ï¸âƒ£ Install Ollama and Pull Models

Download Ollama for your OS:  
ğŸ”— https://ollama.com/download

Then open a terminal or command prompt and run:

```bash
ollama pull llama3             # LLM model
ollama pull nomic-embed-text   # Embedding model
```

Make sure Ollama is running after installation.

### 3ï¸âƒ£ Set Up the Project Environment

```bash
# Clone the repo
git clone https://github.com/MadMando/rag-llm-app.git
cd rag-llm-app

# Create and activate your conda environment
conda create -n rag-env python=3.12 -y
conda activate rag-env

# Install uv and langflow
pip install uv
uv pip install langflow

# Install all other dependencies
uv pip install -r requirements.txt
```

### 4ï¸âƒ£ Start the RAG Stack

```bash
# Start ChromaDB
chroma run --path ./data/chroma_data

# open a new cmd prompt, navigate to rag-llm-app folder and activate env again
cd rag-llm-app
conda activate rag-env

# Start Langflow using uv
uv run langflow run
```

Langflow will open at:  
ğŸ‘‰ http://localhost:7860

### ğŸ“‚ Document Ingestion

Drop your `.pdf` and `.txt` files into the `data/` folder.  
Langflow will use them during your flow execution with ChromaDB.

## ğŸ§© Langflow Project Setup (Step-by-Step)

### ğŸ“ Step 1: Directory Tool
- Drag the **Directory** tool to the canvas.
- Configure it to point to the `data/` directory.

### âœ‚ï¸ Step 2: Text Splitting
- Drag in a **Text Splitter** tool.
- Set `chunk size` to `500` and `chunk overlap` to `100`.
- Connect the Directory output (Data) to the TextSplitter input (Data or Dataframe).

### ğŸ§  Step 3: Embeddings
- Drag in the **Ollama Embeddings** tool.
- Choose `nomic-embed-text` as the model.

### ğŸ“¦ Step 4: Chroma Vector Store
- Drag in the **Chroma Vector Store** tool.
- Set the `Collection Name` to `CFR-1` (or your choice).
- Set the `Persist Directory` to the `data/` folder.
- Connect Chunks from the TextSplitter to Ingest Data in ChromaDB, and Embeddings to Embeddings.

### â–¶ï¸ Run the workflow by pressing the Play button next to the ChromaDB tool.
- This will create your embeddings and load them into the vector store.

### ğŸ’¬ Chat-Ready Flow Setup

### ğŸ§¾ Step 5: Chat Input
- Drag in the **Chat Input** tool.
- Connect it to the **Search Query** input on **Chroma DB**.

### ğŸ” Step 6: Chroma DB Search
- Already configured with collection name `CFR-1` and data path.
- It returns document chunks relevant to the query.

### ğŸ§¹ Step 7: Parser
- Add the **Parser** tool.
- Set it to `text({text})` mode to turn Chroma results into a usable string.
- Connect **Chroma DB â†’ Parser**.

### ğŸ§¾ Step 8: Prompt Template
- Drag in a **Prompt** tool.
- Example template:
  ```
  You are an analyst. Your job is to review provided context and answer questions based on federal, state, or industry-specific compliance rules.

  Context: {context}
  Question: {question}
  ```

### ğŸ¤– Step 9: Ollama (LLM)
- Add the **Ollama** tool.
- Model: `llama3.2-1b`
- Connect **Prompt â†’ Ollama** input.

### ğŸ’¬ Step 10: Chat Output
- Add **Chat Output**.
- Connect **Ollama â†’ Chat Output**.

### ğŸ”— Connections
- Chat Input **Message** â†’ ChromaDB **Search Query**
- Ollama Embeddings **Embeddings** â†’ ChromaDB **Embeddings**
- ChromaDB **DataFrame** â†’ Parser **Data or DataFrame**
- Parser **Parsed Text** â†’ Prompt **context**
- Chat Input **Message** â†’ Prompt **question**
- Prompt **Prompt Message** â†’ Ollama **Input**
- Ollama **Message** â†’ Chat Output **Text**

### â–¶ï¸ Click on Playground (upper left) to test your chat
Ask it specific questions based on your PDFs or text. For more accurate responses, set temperature to `0.1`. For more creative answers, increase it.

## ğŸŒ Sample Web Chat
You can embed the chat interface on a custom webpage using the `<langflow-chat>` component. See example `index.html` in this repo.

## âœ… Status

- âœ… Local LLMs via Ollama  
- âœ… Langflow integration  
- âœ… In-browser chat widget
- â³ Next: Shareable hosted demo + Langflow project exports

## ğŸ“„ License

MIT License

## âœï¸ Author

Built by [Armando Medina](https://www.linkedin.com/in/armandomedina)  
Follow for more projects on GenAI, RAG, and LLM applications.
