# RAG LLM App (Local & Multi-User)

A fully local, multi-user **Retrieval-Augmented Generation (RAG)** application powered by **Langflow**, **ChromaDB**, **Ollama**, and **FastAPI**. This project allows you to load documents, generate embeddings, store in a vector database, and query them using a local LLM â€” all served via an API for concurrent access.

---

## Features

- ğŸ” **Document Search** with embeddings (PDF, TXT)
- ğŸ§  **Local LLM Inference** using [Ollama](https://ollama.com/)
- ğŸ—ƒï¸ **ChromaDB** vector store for fast similarity search
- âš¡ **FastAPI Backend** with support for multiple users
- ğŸ–¼ï¸ **Langflow UI** for building and testing LLM chains
- ğŸ³ **Dockerized** for easy deployment

---

## ğŸ› ï¸ Tech Stack

| Layer            | Tool/Library         |
|------------------|----------------------|
| LLM              | [Ollama](https://ollama.com) (`llama3.2-1b`) |
| Vector DB        | [ChromaDB](https://www.trychroma.com/)     |
| Orchestration    | [Langflow](https://github.com/logspace-ai/langflow) |
| API Layer        | [FastAPI](https://fastapi.tiangolo.com/)   |
| Containerization | Docker               |
| Language         | Python 3.12          |

---

## ğŸ—‚ï¸ Project Structure






